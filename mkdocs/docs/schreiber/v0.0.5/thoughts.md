# Thoughts on R2 and Cloesce Services

Up until now, the only focus for Cloesce has been D1 and Workers. We've created Models, which are abstractions over D1 and Workers (a model represents a table, it's method endpoints).

In v0.0.5, we will introduce Services and Cloudflare R2, both of which are not necessarily tied to a model.

## Services

Workers endpoints do not always need to exist on a model. For instance, if my application exposed only basic REST endpoints, I shouldn't have to create a SQL table and run migrations just to get the program running. We will introduce Services, a group of methods under a namespace, optionally capturing a closure of injected instances.

It will be important that a Cloesce project can run _without_ having a database defined.

For example:

```ts
@Service
export class FooService {
    @POST
    async foo(...) {} // instantiated method

    @GET
    static bar(...) {} // static method

    foo() {} // doesn't have to be exposed as a Worker endpoint
}
```

Exposed static methods will be supported, but maybe we should have a warning along the lines of "why are you doing this", since there isn't really a good reason.

Attributes on a FooService _must_ be dependency injected, and we will assume that is the case by default.

```ts
@Service
export class FooService {
    env: Env;

    async foo(...) {
        this.env.db ...
        this.bar ...
    }

    static bar(...) {
        // static, can't use env or bar
    }
}
```

Services will all be apart of the default dependency injection container, allowing them to be injected in both Service and Model methods.

```ts
@Service
export class FooService {
    env: Env;
    bar: BarService;

    // or equivalently
    func(@Inject bar: BarService) {
        ...
    }
}
```

We will have to detect cyclical service dependencies much like we do with Models, raising an error. We could allow cyclical dependencies through lazy instantiation, but that can be saved for a later time.

### Client Side

All attributes of a Service are injected; all methods will be static on the client class:

```ts
@Service
export class FooService {
  env: Env;
  bar: BarService;

  @POST
  foo() {}
}

// => client code
export class FooService {
  static async foo(): Promise<HttpResult<void>> {...}
}
```

### Cloesce Router Interface with Services

We've defined services in such a way that it aligns closely with how Models are routed.

A new important step will be initializing and injecting all services, meaning we construct every object in topological order and insert it into the DI container.

We will also want to rename the `Model Middleware` to `Namespace Middleware` (or some better name), as it should apply to both Models and Services.

Lastly, Model Hydration will have to be more generic, hydrating a Model or just getting the injected Service.

![Cloesce Router Interface](../../assets/Cloesce%20Router%20Interface%20with%20Services.drawio.png)

## Blobs and R2

R2 is Cloudflares object storage platform. Objects are stored under a namespace called a bucket, with each value having it's own unique key. Keys are decided by the developer, and are not generated by R2.

Files, images, and other kinds of data are classified as blobs (binary large objects). Cloesce operates using Workers, and an important limitation of Workers is that the maximum amount of memory of any instance is 128MB. Thus, uploading a large file or many files through Workers is discouraged, as not to exceed the memory threshold.

R2 is capable of hosting large blobs, so how is data typically uploaded? The answer is through signed uploads and signed downloads. Essentially, a Worker can query the Cloudflare API to ask "give me cryptographic permission to do this file upload/download", and a link will be returned like:

```text
https://bucket.r2.cloudflarestorage.com/uploads/123.png?X-Amz-Expires=600&X-Amz-Signature=abc123...",
```

The client is then responsible for uploading their file to the link. A typical pattern servers follow is having some upload endpoint `foo/upload`, and then having some complete endpoint `foo/upload/complete`, where they can tell the server "I've successfully done an upload". Note that an upload could fail, in which an orphaned R2 blob would exist, forcing some periodic orphan killer to exist.

For the scope of Cloesce, orchestrating the signed upload process seems out of scope. However, there is no reason for us to prohibit blob types for all endpoints, as small files are perfectly fine to go through a Worker proxy. With that in mind, we will want to create an interface that doesn't constrain developers in how they use R2-- be it sending blobs over a worker, or utilizing signed urls.

### Blob CIDL Type & Media Type

We will need to introduce a CIDL Type `Blob`, which indicates to our compiler that this value must be both uploaded (from the client) and received (from the worker) differently than others. `Blob` will be a valid SQL column type.

Currently, incoming data is assumed to be `application/json`, and outgoing data is always `application/json`. JSON is meant for structured data, under a strict text format. File uploads are typically done as `application/octet-stream`, and a combination of the two is done as `multipart/form-data`

#### Only Blob

Let's imagine a method takes a Blob as an argument (assume this is a Cloesce type as well as a TS type):

```ts
@POST
fooBlob(blob: Blob) {}
```

This is simple enough, our client needs to send an octet stream:

```ts
const blob = new Blob([someUint8Array], { type: "application/octet-stream" });

await fetch("/api/upload", {
  method: "POST",
  headers: {
    "Content-Type": "application/octet-stream",
  },
  body: blob,
});
```

Then, our worker must extract the form data and call the method:

```ts
const arrayBuffer = await request.arrayBuffer();
```

#### Blob alongside JSON

It's reasonable to have methods that have both Blobs and JSON, ex:

```ts
@POST
fooBlob(blob: Blob, obj: SomeObj, color: string) {}
```

We will have to use a mix of formData and JSON to accomplish this upload. We will follow the format of putting all JSON data under the FormData key `json`:

```ts
const blob = new Blob(["hello world"], { type: "text/plain" });
const form = new FormData();
form.append("blob", blob, "hello.txt");
form.append("json", JSON.stringify({
    obj: {...}
    color: "..."
}))

const response = await fetch("https://example.com/upload", {
  method: "POST",
  body: form,
});
```

```ts
const data request.formData();
const blob = form.get("blob");
const json = JSON.parse(form.get("json"));
fooBlob(blob, json.obj, json.color);
```

#### Uploading Multiple Blobs & Composition

Given a scenario:

```ts
class Parent {
  blobs: Blob[];
  children: Child[];
}

class Child {
  blobs: Blob[];
  favoriteBlob: Blob;
}
```

How do we serialize the objects into FormData and deserialize appropriately? We can map each blob to an index in a flattened blob array:

```ts
function extractBlobs(value, blobs) {
  if (value instanceof Blob) {
    return { __blobIndex: blobs.push(value) - 1 };
  }

  if (Array.isArray(value)) {
    return value.map((v) => extractBlobs(v, blobs));
  }

  if (value && typeof value === "object") {
    return Object.fromEntries(
      Object.entries(value).map(([k, v]) => [k, extractBlobs(v, blobs)])
    );
  }

  return value;
}

function toJson(obj) {
  const blobs = [];
  const replaced = extractBlobs(obj, blobs);
  return { json: JSON.stringify(replaced), blobs };
}

const { json, blobs } = toJson(parent);
// => now blobs contains [...parent.blobs, ...parent.flatMap(p => p.children.map(c => [c.favoriteBlob, ...c.blobs]))]
//
// and each blob has been replaced with an index ie Child { favoriteBlob: K, blobs: [K + 1, K + 2, K + 3, ...]}
```

When serializing, we should do so with respect to a blob array.

```ts
function parseWithBlobs(json, blobs) {
  function revive(value) {
    if (value && typeof value === "object") {
      if (value.__blobIndex !== undefined) {
        return blobs[value.__blobIndex];
      }

      if (Array.isArray(value)) {
        return value.map((v) => revive(v));
      }

      const out = {};
      for (const k in value) out[k] = revive(value[k]);
      return out;
    }
    return value;
  }

  return revive(JSON.parse(json));
}
```

#### Uploading only JSON

If no Blob type is in the request, we can continue as normal and upload straight JSON.

#### Determining Endpoint Media Type

We will need to define a `MediaType ` in the CIDL: `MediaType::Octet | MediaType::Json | MediaType::FormData`. Each method will have to mark its set of parameters under a `MediaType`, and its return value under a `MediaType`. We can do this in the generator portion, after intaking the `cidl.pre.json`, adding a media type to the end. The backend will then assume that the `MediaType` sent by the frontend is correct (or throw a `415`), and the client will assume that the server is sending the correct `MediaType`. Note that the server could return an error which would be text. if the server responds with the incorrect media type, we will want some fatal error to occur, as the generated code should always be synced (this should only happen if the `CIDL` was tampered with).

### R2

To support R2, we will need to add on to our Wrangler generation. `R2Bucket`'s are the Wrangler API to interact with any R2 bucket. We can generate the toml bindings if buckets are defined. Key information will also be necessary for generating signed download URLs (explained later).

```ts
@WranglerEnv
class Env {
  someBucket: R2Bucket;
  anotherBucket: R2Bucket;
  endpoint: R2Endpoint;
  accessKeyId: R2AccessKeyId;
  secretAccessKey: R2SecretAccessKey;
}
```

producing a wrangler config:

```toml
[[r2_buckets]]
binding = "someBucket"
bucket_name = "someBucket"

[[r2_buckets]]
binding = "anotherBucket"
bucket_name = "anotherBucket"

[[vars]]
endpoint = "xxxx"
```

We will also want to generate a `.dev.vars` for sensitive info:

```toml
R2_ACCESS_KEY_ID="xxxx"
R2_SECRET_ACCESS_KEY="xxxx"
```

Unlike the other wrangler values, we can't validate that the access keys exist. We can validate the endpoint though.

#### Uploads

As talked about before, Cloesce isn't going to orchestrate a signed upload URL process. We will leave that to the user. However, if a developer wishes to not use a signed upload URL, they're always welcome to use the Blob type. For example:

```ts
@D1
class Comment {
  @PrimaryKey
  id: Integer;

  body: string;

  gif: string;

  @POST
  async post(@Inject env: Env, comment: Comment, gif: Blob) {
    await this.env.someBucket.put("my key", gif);

    // ...
    await orm.upsert(Comment, {
      ...comment,
      gif: "my key",
    });
  }
}
```

#### List, Get

Although we aren't orchestrating the signed upload process (for now), we might as well use the signed download process, since creating the download links really has no downside. We can do so by introducing a new navigation property for R2:

```ts
@D1
@CRUD(["GET", "LIST"])
class Comment {
  @PrimaryKey
  id: Integer;

  body: string;

  gif: string;

  @R2({
    bucket: "someBucket",
    keyAttribute: "gif",
    expiresIn: 60 * 15
  })
  gifDownloadUrl: R2Download | undefined;

  @DataSource
  static readonly withDownload: IncludeTree<Comment> {
    gifDownloadUrl: {}
  }
}
```

With this navigation property `gifDownloadUrl` will be replaced with a signed download URL for the key `gif` (should it exist). Like the D1 navigation properties, R2 must be included with an IncludeTree, via a data source. The ORM function `mapSql` will have to populate download URLs if the include tree references them.

Note: Theres a lot of [different options](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/client/s3/command/GetObjectCommand/) available when creating a signed url

```ts
const input = {
  // GetObjectRequest
  Bucket: "STRING_VALUE", // required
  IfMatch: "STRING_VALUE",
  IfModifiedSince: new Date("TIMESTAMP"),
  IfNoneMatch: "STRING_VALUE",
  IfUnmodifiedSince: new Date("TIMESTAMP"),
  Key: "STRING_VALUE", // required
  Range: "STRING_VALUE",
  ResponseCacheControl: "STRING_VALUE",
  ResponseContentDisposition: "STRING_VALUE",
  ResponseContentEncoding: "STRING_VALUE",
  ResponseContentLanguage: "STRING_VALUE",
  ResponseContentType: "STRING_VALUE",
  ResponseExpires: new Date("TIMESTAMP"),
  VersionId: "STRING_VALUE",
  SSECustomerAlgorithm: "STRING_VALUE",
  SSECustomerKey: "STRING_VALUE",
  SSECustomerKeyMD5: "STRING_VALUE",
  RequestPayer: "requester",
  PartNumber: Number("int"),
  ExpectedBucketOwner: "STRING_VALUE",
  ChecksumMode: "ENABLED",
};
```

We will just hardcode these for now with the defaults, but will probably want to expose something to mess with how we generate this at a later time.
